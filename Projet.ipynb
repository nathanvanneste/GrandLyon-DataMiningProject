{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441e008e",
   "metadata": {},
   "source": [
    "# Fouille données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371aa3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn folium scikit-learn mlxtend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dfd51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import time\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors, kneighbors_graph\n",
    "from folium.plugins import FastMarkerCluster\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9c9ecb",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02160af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('flickr_data2.csv', on_bad_lines='skip', sep=\",\")\n",
    "print(\"Nombre de photos au départ :  \" + str(len(df)))\n",
    "\n",
    "# Strip whitespace from column names immediately after loading\n",
    "df.columns = df.columns.str.strip()\n",
    "print(\"Colonnes nettoyées (espaces supprimés)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde8328",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51468cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available columns\n",
    "print(\"Available columns in DataFrame:\")\n",
    "print(df.columns.tolist())\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537aefce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification et affichage des données incohérentes dans df original (avant nettoyage)\n",
    "print(\"Filtrage des données avec dates incohérentes :\")\n",
    "condition_invalide = (\n",
    "    (df['date_taken_hour'] < 0) | (df['date_taken_hour'] > 23) |\n",
    "    (df['date_taken_month'] < 1) | (df['date_taken_month'] > 12) |\n",
    "    (df['date_taken_day'] < 1) | (df['date_taken_day'] > 31)\n",
    ")\n",
    "df_incoherentes = df[condition_invalide]\n",
    "print(f\"Nombre total de données incohérentes : {len(df_incoherentes)}\")\n",
    "print(\"Affichage du tableau des données incohérentes :\")\n",
    "display(df_incoherentes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea9401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse profonde des données incohérentes\n",
    "print(\"Analyse des données incohérentes :\")\n",
    "print(f\"Colonnes disponibles : {list(df_incoherentes.columns)}\")\n",
    "\n",
    "# Statistiques descriptives des colonnes de date pour les incohérentes\n",
    "print(\"\\nStatistiques des colonnes de date dans les données incohérentes :\")\n",
    "date_cols = ['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']\n",
    "print(df_incoherentes[date_cols].describe())\n",
    "\n",
    "# Vérifier les types de données\n",
    "print(\"\\nTypes de données dans df_incoherentes :\")\n",
    "print(df_incoherentes[date_cols].dtypes)\n",
    "\n",
    "# Examiner quelques exemples spécifiques\n",
    "print(\"\\nExemples de lignes incohérentes (premières 10) :\")\n",
    "display(df_incoherentes[date_cols].head(10))\n",
    "\n",
    "# Chercher des patterns : par exemple, heures >23, voir si elles pourraient être des minutes\n",
    "heures_sup_23 = df_incoherentes[df_incoherentes['date_taken_hour'] > 23]\n",
    "print(f\"\\nLignes avec heure >23 : {len(heures_sup_23)}\")\n",
    "if len(heures_sup_23) > 0:\n",
    "    print(\"Valeurs d'heure >23 :\")\n",
    "    print(heures_sup_23['date_taken_hour'].unique())\n",
    "    print(\"Exemples :\")\n",
    "    display(heures_sup_23[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(5))\n",
    "\n",
    "# Mois >12\n",
    "mois_sup_12 = df_incoherentes[df_incoherentes['date_taken_month'] > 12]\n",
    "print(f\"\\nLignes avec mois >12 : {len(mois_sup_12)}\")\n",
    "if len(mois_sup_12) > 0:\n",
    "    print(\"Valeurs de mois >12 :\")\n",
    "    print(mois_sup_12['date_taken_month'].unique())\n",
    "    print(\"Exemples :\")\n",
    "    display(mois_sup_12[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(5))\n",
    "\n",
    "# Jours >31\n",
    "jours_sup_31 = df_incoherentes[df_incoherentes['date_taken_day'] > 31]\n",
    "print(f\"\\nLignes avec jour >31 : {len(jours_sup_31)}\")\n",
    "if len(jours_sup_31) > 0:\n",
    "    print(\"Valeurs de jour >31 :\")\n",
    "    print(jours_sup_31['date_taken_day'].unique())\n",
    "    print(\"Exemples :\")\n",
    "    display(jours_sup_31[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(5))\n",
    "\n",
    "# Vérifier si les valeurs semblent décalées (par exemple, heure dans minute, etc.)\n",
    "print(\"\\nVérification de décalage possible :\")\n",
    "# Par exemple, si heure >23, voir si minute est valide\n",
    "if len(heures_sup_23) > 0:\n",
    "    print(\"Pour les heures >23, vérifier les minutes :\")\n",
    "    print(heures_sup_23['date_taken_minute'].describe())\n",
    "\n",
    "# De même pour mois\n",
    "if len(mois_sup_12) > 0:\n",
    "    print(\"Pour les mois >12, vérifier les jours :\")\n",
    "    print(mois_sup_12['date_taken_day'].describe())\n",
    "\n",
    "# Chercher des NaN ou valeurs manquantes\n",
    "print(f\"\\nNombre de NaN par colonne dans les incohérentes :\")\n",
    "print(df_incoherentes[date_cols].isnull().sum())\n",
    "\n",
    "# Voir si les valeurs sont des floats ou strings étranges\n",
    "print(\"\\nExemples de valeurs uniques pour diagnostiquer :\")\n",
    "for col in date_cols:\n",
    "    unique_vals = df_incoherentes[col].dropna().unique()\n",
    "    print(f\"{col} : {unique_vals[:10]}... (total unique: {len(unique_vals)})\")  # Premiers 10 pour aperçu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642e978",
   "metadata": {},
   "source": [
    "# Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf53c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "df_clean = df.dropna(axis=1, how='all')\n",
    "\n",
    "cols_date = ['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']\n",
    "\n",
    "df_clean = df_clean.drop_duplicates(subset=['user', 'lat', 'long'] + cols_date)\n",
    "df_clean = df_clean.dropna(subset=cols_date)\n",
    "\n",
    "for col in cols_date:\n",
    "    df_clean[col] = df_clean[col].astype(int)\n",
    "\n",
    "colonnes_utiles = ['id', 'user', 'lat', 'long', 'tags', 'title'] + cols_date\n",
    "df_clean = df_clean[colonnes_utiles]\n",
    "\n",
    "print(\"\\nDonnées nettoyées :\")\n",
    "display(df_clean.head())\n",
    "print(\"Nombre de photos restantes : \" + str(len(df_clean)))\n",
    "\n",
    "pourcentage_supprimes = ((len(df) - len(df_clean)) / len(df)) * 100\n",
    "print(f\"Pourcentage de données supprimées : {pourcentage_supprimes:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a7cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Année min : {df_clean['date_taken_year'].min()}\")\n",
    "print(f\"Année max : {df_clean['date_taken_year'].max()}\")\n",
    "\n",
    "df_clean = df_clean[\n",
    "    (df_clean['date_taken_year'] >= 2004) &\n",
    "    (df_clean['date_taken_year'] <= 2026)\n",
    "]\n",
    "\n",
    "print(f\"Année min : {df_clean['date_taken_year'].min()}\")\n",
    "print(f\"Année max : {df_clean['date_taken_year'].max()}\")\n",
    "print(\"Après filtre temporel : \" + str(len(df_clean)) + \" photos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aac3b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs invalides dans les colonnes de date\n",
    "print(\"Vérification des heures invalides (pas entre 0 et 23) :\")\n",
    "heures_invalides = df_clean[(df_clean['date_taken_hour'] < 0) | (df_clean['date_taken_hour'] > 23)]\n",
    "print(f\"Nombre d'heures invalides : {len(heures_invalides)}\")\n",
    "if len(heures_invalides) > 0:\n",
    "    display(heures_invalides[['id', 'date_taken_hour']].head(10))  # Afficher les 10 premières pour lisibilité\n",
    "\n",
    "print(\"\\nVérification des mois invalides (pas entre 1 et 12) :\")\n",
    "mois_invalides = df_clean[(df_clean['date_taken_month'] < 1) | (df_clean['date_taken_month'] > 12)]\n",
    "print(f\"Nombre de mois invalides : {len(mois_invalides)}\")\n",
    "if len(mois_invalides) > 0:\n",
    "    display(mois_invalides[['id', 'date_taken_month']].head(10))\n",
    "\n",
    "print(\"\\nVérification des jours invalides (pas entre 1 et 31) :\")\n",
    "jours_invalides = df_clean[(df_clean['date_taken_day'] < 1) | (df_clean['date_taken_day'] > 31)]\n",
    "print(f\"Nombre de jours invalides : {len(jours_invalides)}\")\n",
    "if len(jours_invalides) > 0:\n",
    "    display(jours_invalides[['id', 'date_taken_day']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335bea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative de correction automatique du décalage de colonnes\n",
    "def corriger_decalage(row):\n",
    "    # Copie des valeurs actuelles\n",
    "    y, m, d, h, mn = row['date_taken_year'], row['date_taken_month'], row['date_taken_day'], row['date_taken_hour'], row['date_taken_minute']\n",
    "\n",
    "    # Si minute est une année (entre 2000 et 2030), décalage vers la droite détecté\n",
    "    if pd.notna(mn) and 2000 <= mn <= 2030:\n",
    "        # Décalage : year <- minute, minute <- hour, hour <- day, day <- month, month <- year\n",
    "        new_y = int(mn)\n",
    "        new_mn = h % 60  # minute de 0-59\n",
    "        new_h = d % 24   # heure de 0-23\n",
    "        new_d = m % 31   # jour approximatif 1-31\n",
    "        new_m = y % 12   # mois 1-12\n",
    "        if new_m == 0: new_m = 12\n",
    "        if new_d == 0: new_d = 1\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Si month >12 et semble être une année, décalage différent\n",
    "    elif m > 12 and m < 2030:\n",
    "        # Suppose month est year, alors décalage inverse\n",
    "        new_y = int(m)\n",
    "        new_m = y % 12\n",
    "        if new_m == 0: new_m = 12\n",
    "        new_d = d % 31\n",
    "        if new_d == 0: new_d = 1\n",
    "        new_h = h % 24\n",
    "        new_mn = mn if pd.notna(mn) else 0\n",
    "        new_mn = new_mn % 60\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Si day >31 et semble année\n",
    "    elif d > 31 and d < 2030:\n",
    "        new_y = int(d)\n",
    "        new_d = m % 31\n",
    "        if new_d == 0: new_d = 1\n",
    "        new_m = y % 12\n",
    "        if new_m == 0: new_m = 12\n",
    "        new_h = h % 24\n",
    "        new_mn = mn if pd.notna(mn) else 0\n",
    "        new_mn = new_mn % 60\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Sinon, essayer de corriger seulement les invalides\n",
    "    else:\n",
    "        new_y = y\n",
    "        new_m = m if 1 <= m <= 12 else (m % 12) if m % 12 != 0 else 12\n",
    "        new_d = d if 1 <= d <= 31 else (d % 31) if d % 31 != 0 else 1\n",
    "        new_h = h if 0 <= h <= 23 else (h % 24)\n",
    "        new_mn = mn if pd.notna(mn) and 0 <= mn <= 59 else (mn % 60) if pd.notna(mn) else 0\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "# Appliquer la correction\n",
    "df_incoherentes_corrected = df_incoherentes.copy()\n",
    "df_incoherentes_corrected[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']] = df_incoherentes.apply(corriger_decalage, axis=1)\n",
    "\n",
    "print(\"Après correction automatique :\")\n",
    "print(\"Exemples corrigés :\")\n",
    "display(df_incoherentes_corrected[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(10))\n",
    "\n",
    "# Vérifier combien sont maintenant valides\n",
    "condition_valide_apres = (\n",
    "    (df_incoherentes_corrected['date_taken_hour'] >= 0) & (df_incoherentes_corrected['date_taken_hour'] <= 23) &\n",
    "    (df_incoherentes_corrected['date_taken_month'] >= 1) & (df_incoherentes_corrected['date_taken_month'] <= 12) &\n",
    "    (df_incoherentes_corrected['date_taken_day'] >= 1) & (df_incoherentes_corrected['date_taken_day'] <= 31)\n",
    ")\n",
    "valides_apres = df_incoherentes_corrected[condition_valide_apres]\n",
    "print(f\"\\nNombre de lignes maintenant valides après correction : {len(valides_apres)} / {len(df_incoherentes_corrected)}\")\n",
    "\n",
    "# Afficher les lignes corrigées valides\n",
    "if len(valides_apres) > 0:\n",
    "    print(\"Exemples de lignes corrigées et maintenant valides :\")\n",
    "    display(valides_apres[['id', 'date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58939fc",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "# Vérifier que les coordonnées géographiques sont cohérentes.\n",
    "photos_avant_filtre = len(df_clean)\n",
    "\n",
    "df_clean = df_clean[\n",
    "    (df_clean['lat'] >= -90) & (df_clean['lat'] <= 90) &\n",
    "    (df_clean['long'] >= -180) & (df_clean['long'] <= 180)\n",
    "]\n",
    "photos_supprimees = photos_avant_filtre - len(df_clean)\n",
    "print(f\"Photos supprimées par le filtre géographique : {photos_supprimees}\")\n",
    "print(f\"Photos avec coordonnées valides : {len(df_clean)}\")\n",
    "print(f\"Min/Max Latitude  : {df_clean['lat'].min()} / {df_clean['lat'].max()}\")\n",
    "print(f\"Min/Max Longitude : {df_clean['long'].min()} / {df_clean['long'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c48ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tentative de correction automatique du décalage de colonnes\n",
    "def corriger_decalage(row):\n",
    "    # Copie des valeurs actuelles\n",
    "    y, m, d, h, mn = row['date_taken_year'], row['date_taken_month'], row['date_taken_day'], row['date_taken_hour'], row['date_taken_minute']\n",
    "\n",
    "    # Si minute est une année (entre 2000 et 2030), décalage vers la droite détecté\n",
    "    if pd.notna(mn) and 2000 <= mn <= 2030:\n",
    "        # Décalage : year <- minute, minute <- hour, hour <- day, day <- month, month <- year\n",
    "        new_y = int(mn)\n",
    "        new_mn = h % 60  # minute de 0-59\n",
    "        new_h = d % 24   # heure de 0-23\n",
    "        new_d = m % 31   # jour approximatif 1-31\n",
    "        new_m = y % 12   # mois 1-12\n",
    "        if new_m == 0: new_m = 12\n",
    "        if new_d == 0: new_d = 1\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Si month >12 et semble être une année, décalage différent\n",
    "    elif m > 12 and m < 2030:\n",
    "        # Suppose month est year, alors décalage inverse\n",
    "        new_y = int(m)\n",
    "        new_m = y % 12\n",
    "        if new_m == 0: new_m = 12\n",
    "        new_d = d % 31\n",
    "        if new_d == 0: new_d = 1\n",
    "        new_h = h % 24\n",
    "        new_mn = mn if pd.notna(mn) else 0\n",
    "        new_mn = new_mn % 60\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Si day >31 et semble année\n",
    "    elif d > 31 and d < 2030:\n",
    "        new_y = int(d)\n",
    "        new_d = m % 31\n",
    "        if new_d == 0: new_d = 1\n",
    "        new_m = y % 12\n",
    "        if new_m == 0: new_m = 12\n",
    "        new_h = h % 24\n",
    "        new_mn = mn if pd.notna(mn) else 0\n",
    "        new_mn = new_mn % 60\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "    # Sinon, essayer de corriger seulement les invalides\n",
    "    else:\n",
    "        new_y = y\n",
    "        new_m = m if 1 <= m <= 12 else (m % 12) if m % 12 != 0 else 12\n",
    "        new_d = d if 1 <= d <= 31 else (d % 31) if d % 31 != 0 else 1\n",
    "        new_h = h if 0 <= h <= 23 else (h % 24)\n",
    "        new_mn = mn if pd.notna(mn) and 0 <= mn <= 59 else (mn % 60) if pd.notna(mn) else 0\n",
    "        return pd.Series([new_y, new_m, new_d, new_h, new_mn])\n",
    "\n",
    "# Appliquer la correction\n",
    "df_incoherentes_corrected = df_incoherentes.copy()\n",
    "df_incoherentes_corrected[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']] = df_incoherentes.apply(corriger_decalage, axis=1)\n",
    "\n",
    "print(\"Après correction automatique :\")\n",
    "print(\"Exemples corrigés :\")\n",
    "display(df_incoherentes_corrected[['date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(10))\n",
    "\n",
    "# Vérifier combien sont maintenant valides\n",
    "condition_valide_apres = (\n",
    "    (df_incoherentes_corrected['date_taken_hour'] >= 0) & (df_incoherentes_corrected['date_taken_hour'] <= 23) &\n",
    "    (df_incoherentes_corrected['date_taken_month'] >= 1) & (df_incoherentes_corrected['date_taken_month'] <= 12) &\n",
    "    (df_incoherentes_corrected['date_taken_day'] >= 1) & (df_incoherentes_corrected['date_taken_day'] <= 31)\n",
    ")\n",
    "valides_apres = df_incoherentes_corrected[condition_valide_apres]\n",
    "print(f\"\\nNombre de lignes maintenant valides après correction : {len(valides_apres)} / {len(df_incoherentes_corrected)}\")\n",
    "\n",
    "# Afficher les lignes corrigées valides\n",
    "if len(valides_apres) > 0:\n",
    "    print(\"Exemples de lignes corrigées et maintenant valides :\")\n",
    "    display(valides_apres[['id', 'date_taken_year', 'date_taken_month', 'date_taken_day', 'date_taken_hour', 'date_taken_minute']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d57108",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nettoyage supplémentaire : suppression de doublons spatiaux par utilisateur (rayon 2m)\n",
    "import math\n",
    "\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    # Distance en mètres approximée\n",
    "    R = 6371000  # Rayon de la Terre en mètres\n",
    "    dlat = math.radians(lat2 - lat1)\n",
    "    dlon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(dlat/2)**2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def supprimer_doublons_spatiaux_par_user(df, rayon_m=2):\n",
    "    df_result = []\n",
    "    for user, group in df.groupby('user'):\n",
    "        group = group.sort_values('id')\n",
    "        garde = []\n",
    "        for idx, row in group.iterrows():\n",
    "            lat, lon = row['lat'], row['long']\n",
    "            trop_proche = False\n",
    "            for g_lat, g_lon in garde:\n",
    "                if haversine_distance(lat, lon, g_lat, g_lon) < rayon_m:\n",
    "                    trop_proche = True\n",
    "                    break\n",
    "            if not trop_proche:\n",
    "                garde.append((lat, lon))\n",
    "                df_result.append(row)\n",
    "    return pd.DataFrame(df_result)\n",
    "\n",
    "photos_avant_spatial = len(df_clean)\n",
    "print(f\"Photos avant nettoyage spatial : {photos_avant_spatial}\")\n",
    "\n",
    "df_clean = supprimer_doublons_spatiaux_par_user(df_clean, rayon_m=2)\n",
    "\n",
    "photos_apres_spatial = len(df_clean)\n",
    "photos_supprimees_spatial = photos_avant_spatial - photos_apres_spatial\n",
    "print(f\"Photos supprimées par nettoyage spatial : {photos_supprimees_spatial}\")\n",
    "print(f\"Photos restantes après nettoyage spatial : {photos_apres_spatial}\")\n",
    "print(f\"Pourcentage supprimé par spatial : {photos_supprimees_spatial / photos_avant_spatial * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ed60de",
   "metadata": {},
   "source": [
    "# Affichage de la carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b0cf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la carte centrée sur une latitude et une longitude moyennes\n",
    "m = folium.Map(location=[45.7640, 4.8357], zoom_start=13)\n",
    "locations = list(zip(df['lat'], df['long']))\n",
    "FastMarkerCluster(data=locations).add_to(m)\n",
    "# Sauvegarde la carte dans un fichier HTML\n",
    "m.save('ma_carte_lyon_all_photos.html')\n",
    "\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47551d5d",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93009ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clustering = df_clean[['lat', 'long']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a941a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clustering)\n",
    "\n",
    "# show\n",
    "scaled_data_df = pd.DataFrame(data=scaled_data, columns=['lat', 'long'])\n",
    "\n",
    "print(\"Aperçu des données standardisées (prêtes pour l'algo) :\")\n",
    "display(scaled_data_df.head())\n",
    "\n",
    "print(f\"Moyennes : \\n{scaled_data_df.mean()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa513a",
   "metadata": {},
   "source": [
    "## Estimation du nombre de clusters pour K-means via la méthode du coude "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ce3607",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "k_range = range(1, 50)\n",
    "\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)\n",
    "    kmeans.fit(scaled_data_df)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Création du graphique\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(k_range, inertia, marker='o')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.xticks(k_range)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf1de30",
   "metadata": {},
   "source": [
    "Au vu des résultats, nous pouvons partir sur une valeur entre 15 et 20. Cela permet d'avoir suffisant de zones d'intérêt pour une grande ville comme Lyon, tout en évitant de diviser les zones d'intéret en plusieurs cluster. Nous choisisons donc une valeur de 17 clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3415aa13",
   "metadata": {},
   "source": [
    "## Utilisation de l'algorithme K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1cd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=100, init='k-means++', random_state=42)\n",
    "\n",
    "kmeans.fit(scaled_data_df)\n",
    "\n",
    "df_clean['cluster'] = kmeans.labels_\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6579e4",
   "metadata": {},
   "source": [
    "## Affichage des clusters sur la carte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc8e635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generer_carte_clusters(df, nom_fichier=\"ma_carte.html\"):\n",
    "    \"\"\"\n",
    "    Génère une carte Folium avec des zones colorées selon la densité (Convex Hull).\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_data = df[df['cluster'] != -1]\n",
    "    \n",
    "    if filtered_data.empty:\n",
    "        print(\"⚠️ Attention : Aucun cluster valide trouvé (tout est bruit ou vide).\")\n",
    "        return None\n",
    "\n",
    "    counts = filtered_data['cluster'].value_counts()\n",
    "    \n",
    "    if len(counts) == 0:\n",
    "        print(\"Pas de données à afficher.\")\n",
    "        return None\n",
    "        \n",
    "    min_val = counts.min()\n",
    "    max_val = counts.max()\n",
    "    \n",
    "    print(f\"--- Génération de {nom_fichier} ---\")\n",
    "    print(f\"Cluster min : {min_val} photos | Cluster max : {max_val} photos\")\n",
    "\n",
    "    colors = [\"white\", \"blue\", \"red\", \"darkred\"]\n",
    "    cmap = mcolors.LinearSegmentedColormap.from_list(\"mon_degrade\", colors)\n",
    "    \n",
    "    # Normalisation\n",
    "    if min_val == max_val:\n",
    "        norm = mcolors.Normalize(vmin=min_val, vmax=max_val)\n",
    "    else:\n",
    "        norm = mcolors.LogNorm(vmin=min_val, vmax=max_val)\n",
    "\n",
    "    def get_color(n):\n",
    "        return mcolors.to_hex(cmap(norm(n)))\n",
    "\n",
    "    center_lat = df['lat'].mean()\n",
    "    center_long = df['long'].mean()\n",
    "    m = folium.Map(location=[center_lat, center_long], zoom_start=13)\n",
    "\n",
    "    clusters_ids = sorted(df['cluster'].unique())\n",
    "\n",
    "    for cluster_id in clusters_ids:\n",
    "        if cluster_id == -1: \n",
    "            continue\n",
    "\n",
    "        points = df[df['cluster'] == cluster_id][['lat', 'long']].values\n",
    "        n_photos = len(points)\n",
    "        \n",
    "        color = get_color(n_photos)\n",
    "        \n",
    "        opacity = 0.3 + (norm(n_photos) * 0.5) if max_val > min_val else 0.5\n",
    "\n",
    "        if len(points) >= 3:\n",
    "            try:\n",
    "                hull = ConvexHull(points)\n",
    "                points_contour = points[hull.vertices]\n",
    "                \n",
    "                folium.Polygon(\n",
    "                    locations=points_contour,\n",
    "                    color=color,\n",
    "                    weight=2,\n",
    "                    fill=True,\n",
    "                    fill_color=color,\n",
    "                    fill_opacity=opacity,\n",
    "                    popup=f\"Zone {cluster_id}: {n_photos} photos\"\n",
    "                ).add_to(m)\n",
    "                \n",
    "                center = np.mean(points, axis=0)\n",
    "                style_texte = \"color: black; text-shadow: 1px 1px 0px white; font-weight: bold; font-size: 10pt;\"\n",
    "                folium.Marker(\n",
    "                    center,\n",
    "                    icon=folium.DivIcon(html=f'<div style=\"{style_texte}\">{n_photos}</div>')\n",
    "                ).add_to(m)\n",
    "                \n",
    "            except Exception:\n",
    "                pass\n",
    "                \n",
    "        else:\n",
    "            for pt in points:\n",
    "                folium.CircleMarker(\n",
    "                    pt, \n",
    "                    radius=5, \n",
    "                    color=color, \n",
    "                    fill=True, \n",
    "                    fill_color=color,\n",
    "                    popup=f\"Cluster {cluster_id}\"\n",
    "                ).add_to(m)\n",
    "\n",
    "    m.save(nom_fichier)\n",
    "    print(f\"Carte sauvegardée avec succès : {nom_fichier}\\n\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c28531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generer_carte_clusters(df_clean, \"carte_k_means.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a98502",
   "metadata": {},
   "source": [
    "On voit que certaines zones sont bien plus denses que d'autres. En tant que connaisseurs de la ville de Lyon, nous reconnaissons bien évidemment que que le centre-ville et le vieux-Lyon sont les endroits les plus photographiés, ce qui parait cohérent. \n",
    "\n",
    "Cependant, K-means nous permet seulement de voir de grandes zones, ce qui n'est clairement pas ce que nous cherchons à faire pour déterminer des petits clusters d'évènements. Nous devons trouver d'autres algorithmes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9a5bca",
   "metadata": {},
   "source": [
    "## Utilisation de l'algorithme Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63004874",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df = scaled_data_df.sample(n=3000, random_state=42)\n",
    "\n",
    "print(f\"Recherche des paramètres optimaux sur {len(subset_df)} points...\")\n",
    "\n",
    "linkage_methods = ['ward', 'average']\n",
    "range_n_clusters = range(10, 1000, 10)\n",
    "\n",
    "scores = {'ward': [], 'average': []}\n",
    "\n",
    "for method in linkage_methods:\n",
    "    print(f\"Test de la méthode : {method}...\")\n",
    "    for k in range_n_clusters:\n",
    "        model = AgglomerativeClustering(n_clusters=k, linkage=method)        \n",
    "        labels = model.fit_predict(subset_df)\n",
    "        \n",
    "        # On calcule le score pour déterminer la qualité de la séparation\n",
    "        score = silhouette_score(subset_df, labels)\n",
    "        scores[method].append(score)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Courbe Ward\n",
    "plt.plot(range_n_clusters, scores['ward'], label='Ward (Compact)', marker='o', color='blue')\n",
    "# Courbe Average\n",
    "plt.plot(range_n_clusters, scores['average'], label='Average (Naturel)', marker='x', color='green', linestyle='--')\n",
    "\n",
    "plt.title(\"Comparaison ward vs Average\")\n",
    "plt.xlabel(\"Nombre de Clusters (k)\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# meilleurs résulats\n",
    "best_ward_score = max(scores['ward'])\n",
    "best_ward_k = range_n_clusters[scores['ward'].index(best_ward_score)]\n",
    "\n",
    "print(f\"Meilleur config Ward : k={best_ward_k} (Score: {best_ward_score:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d73195",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "LINKAGE_GAGNANT = 'ward' \n",
    "N_CLUSTERS = 250\n",
    "\n",
    "print(f\"Lancement du Clustering Hiérarchique ({LINKAGE_GAGNANT}) pour {N_CLUSTERS} zones...\")\n",
    "\n",
    "print(\"Calcul du graphe de connectivité...\")\n",
    "connectivity = kneighbors_graph(scaled_data_df, n_neighbors=10, include_self=False)\n",
    "\n",
    "model = AgglomerativeClustering(\n",
    "    n_clusters=N_CLUSTERS, \n",
    "    connectivity=connectivity, \n",
    "    linkage=LINKAGE_GAGNANT\n",
    ")\n",
    "\n",
    "print(\"Entraînement du modèle (patience)...\")\n",
    "start = time.time()\n",
    "# fit_predict calcule les groupes directement\n",
    "labels = model.fit_predict(scaled_data_df) \n",
    "end = time.time()\n",
    "print(f\"Terminé en {end - start:.2f} secondes !\")\n",
    "\n",
    "df_clean['cluster'] = labels\n",
    "\n",
    "nom_fichier = f\"carte_hierarchique_{LINKAGE_GAGNANT}.html\"\n",
    "generer_carte_clusters(df_clean, nom_fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d57930",
   "metadata": {},
   "source": [
    "## Utilisation de l'algorithme DBSCAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d93801",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = 30\n",
    "\n",
    "# On regarde la distance vers le k-ième voisin\n",
    "neighbors = NearestNeighbors(n_neighbors=min_samples)\n",
    "neighbors_fit = neighbors.fit(scaled_data_df)\n",
    "distances, indices = neighbors_fit.kneighbors(scaled_data_df)\n",
    "\n",
    "distances = np.sort(distances[:, min_samples-1], axis=0)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(distances)\n",
    "plt.title(\"Graphe des k-distances (pour trouver eps)\")\n",
    "plt.xlabel(\"Points triés\")\n",
    "plt.ylabel(\"Epsilon (Distance)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c006844b",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_samples = range(30, 80, 5)\n",
    "eps = np.arange(0.002, 0.06, 0.002)\n",
    "output = []\n",
    "\n",
    "for ms in min_samples:\n",
    "    for ep in eps:\n",
    "        labels = DBSCAN(min_samples=ms, eps = ep).fit(scaled_data_df).labels_\n",
    "        score = silhouette_score(scaled_data_df, labels, sample_size=5000, random_state=42)\n",
    "        output.append((ms, ep, score))\n",
    "\n",
    "\n",
    "min_samples, eps, score = sorted(output, key=lambda x:x[-1])[-1]\n",
    "print(f\"Best silhouette_score: {score}\")\n",
    "print(f\"min_samples: {min_samples}\")\n",
    "print(f\"eps: {eps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d168e547",
   "metadata": {},
   "source": [
    "Au vu du résultat lorsque nous utilisons ces valeurs, il semble que le epsilon est trop grand. En effet, nous avons encore des clusters trop grands. Cependant le min_samples semble cohérent. Pour pouvoir différencier clairement les zones d'affluence, il faudrait un epsilon bien plus petit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3fd8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_CHOISI = 0.058\n",
    "MIN_SAMPLES = 45\n",
    "\n",
    "print(f\"Lancement de DBSCAN (eps={EPSILON_CHOISI}, min={MIN_SAMPLES})...\")\n",
    "\n",
    "dbscan = DBSCAN(eps=EPSILON_CHOISI, min_samples=MIN_SAMPLES)\n",
    "clusters = dbscan.fit_predict(scaled_data_df)\n",
    "\n",
    "df_clean['cluster'] = clusters\n",
    "\n",
    "n_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)\n",
    "len(clusters)\n",
    "n_noise = list(clusters).count(-1)\n",
    "percent_noise = (n_noise / len(df_clean)) * 100\n",
    "\n",
    "print(f\"--> Résultat : {n_clusters} clusters trouvés.\")\n",
    "print(f\"--> Bruit : {n_noise} photos ignorées ({percent_noise:.1f}%)\")\n",
    "\n",
    "nom_fichier = f\"carte_dbscan.html\"\n",
    "generer_carte_clusters(df_clean, nom_fichier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b212f",
   "metadata": {},
   "source": [
    "## Utilisation de l'algorithme Apriori sur les tags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d169ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupérer les clusters uniques (exclure le bruit -1 si présent)\n",
    "clusters_uniques = sorted([c for c in df_clean['cluster'].unique() if c != -1])\n",
    "\n",
    "print(f\"\\nAnalyse Apriori pour {len(clusters_uniques)} clusters\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "resultats_apriori = {}\n",
    "\n",
    "for cluster_id in clusters_uniques:\n",
    "    print(f\"Cluster {cluster_id}...\", end=\" \", flush=True)\n",
    "    \n",
    "    # Filtrer les données du cluster\n",
    "    cluster_data = df_clean[df_clean['cluster'] == cluster_id]\n",
    "    \n",
    "    if len(cluster_data) == 0:\n",
    "        print(\"vide\")\n",
    "        continue\n",
    "    \n",
    "    # Créer liste de transactions (tags par photo)\n",
    "    transactions = []\n",
    "    for tags_str in cluster_data['tags']:\n",
    "        if pd.notna(tags_str) and tags_str != '':\n",
    "            tags_list = [tag.strip() for tag in str(tags_str).split(',') if tag.strip()]\n",
    "            if tags_list:\n",
    "                transactions.append(tags_list)\n",
    "    \n",
    "    if not transactions:\n",
    "        print(\"pas de tags\")\n",
    "        continue\n",
    "    \n",
    "    # Encoder et appliquer Apriori\n",
    "    te = TransactionEncoder()\n",
    "    te_ary = te.fit(transactions).transform(transactions)\n",
    "    df_encoded = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    \n",
    "    # Support plus haut pour éviter la surcharge\n",
    "    frequent_itemsets = apriori(df_encoded, min_support=0.2, use_colnames=True)\n",
    "    \n",
    "    # Récupérer les tags fréquents\n",
    "    frequent_words = frequent_itemsets[frequent_itemsets['itemsets'].apply(lambda x: len(x) == 1)]\n",
    "    frequent_words = frequent_words.sort_values('support', ascending=False).head(5)\n",
    "    \n",
    "    resultats_apriori[cluster_id] = frequent_words\n",
    "    \n",
    "    # Afficher résultats\n",
    "    print(f\"({len(cluster_data)} photos)\")\n",
    "    if len(frequent_words) > 0:\n",
    "        for _, row in frequent_words.iterrows():\n",
    "            tag = list(row['itemsets'])[0]\n",
    "            print(f\"  - {tag}: {row['support']:.1%}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nTerminé: {len(resultats_apriori)} clusters analysés\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
